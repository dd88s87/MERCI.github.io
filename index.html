<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards">
  <meta name="keywords" content="Large Language Models, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fudan.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bibtex-js@2.0.0/dist/bibtex.min.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xuan Zhang</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              Ruixiao Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Zhijian Zhou</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              Long Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Yulei Qin</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Ke Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Xing Sun</a><sup>3</sup>
            </span>
            <span class="author-block">
              Xiaoyu Tan</a><sup>3</sup><sup>†</sup>
            </span>
            <span class="author-block">
              Chao Qu</a><sup>1,2</sup><sup>†</sup>
            </span>
            <span class="author-block">
              Yuan Qi</a><sup>1</sup><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>3</sup>Tencent Youtu Lab</span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-4">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL) has enabled LLMs to tackle challenging domains such as mathematics and code generation, but sparse rewards and inefficient exploration remain major bottlenecks. Existing methods like GRPO and DAPO remove the need for value functions and provide token-level variability, yet they lack temporally consistent exploration over long reasoning trajectories. To guide exploration in such frameworks, many prevalent techniques rely on entropy regularization to encourage local policy diversity. While effective, this approach is limited for complex, long-horizon tasks. We see an opportunity to design complementary strategies that provide more directed, temporally-consistent exploration signals particularly for those tasks, motivating our investigation into principled exploration strategies compatible with modern value-free RL.
          </p>
          <p>
            Traditional exploration strategies—such as entropy regularization, curiosity-driven bonuses, or ensemble-based uncertainty estimates—either fail to provide temporally consistent signals, incur prohibitive computational costs, or break down at LLM scale.
          </p>
          <p>
            Our work addresses this gap with a critical insight: autoregressive LLM reasoning operates in a deterministic environment: each token uniquely determines the next state. This dramatically simplifies the Uncertainty Bellman Equation, reducing the intractable challenge of estimating Q-value uncertainty to the more manageable task of estimating local reward uncertainty. Building on this, we introduce MERCI—the first principled deep exploration algorithm for LLM reasoning. MERCI leverages the lightweight and theoretically grounded <i>Flipping Coins pseudo-count</i> method to estimate state novelty, translating it into intrinsic rewards that guide policy optimization.
          </p>
        </div>
        <h3 class="title is-4">Contributions</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>A Novel Theoretical Framework for LLM Exploration.</b> We establish a new framework based on a key insight: the LLM's known and deterministic transition dynamics simplify the Uncertainty Bellman Equation. This renders principled, uncertainty-driven exploration tractable at scale by reducing the intractable problem of Q-value uncertainty to a manageable estimation of local reward uncertainty.
            </li>
            <li>
              <b>A Practical and Scalable Exploration Algorithm.</b> We propose <b>MERCI</b>, a novel algorithm that operationalizes our theoretical framework. MERCI employs a highly scalable counting method to translate state novelty into a potent intrinsic reward signal, designed for seamless integration with modern, value-free policy optimization methods like GRPO.
            </li>
            <li>
              <b>State-of-the-Art Performance on Complex Reasoning.</b> Our extensive empirical evidence on challenging reasoning benchmarks, including MATH and SQL generation, demonstrate that MERCI beats strong baselines. Its directed exploration mechanism mitigates premature convergence and leads to the discovery of more robust and accurate solutions.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png"
                class="method-image"/>
          <p style="text-align:center"><strong>Figure 1:</strong> Method Overview: MERCI</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Methodology</h2>
        <h4 class="title is-4">The Uncertainty Bellman Equation with Known Transitions</h4>
        <div class="content has-text-justified">
          <p>
            The Uncertainty Bellman Equation (UBE) provides a principled mechanism for propagating epistemic uncertainty---quantified as the variance of the posterior distribution over Q-values---through time.  For clarity, we will use the terms "uncertainty" and "variance" interchangeably throughout this section. Our core theoretical contribution stems from a key insight: <b>the Markov Decision Process (MDP) underlying LLM reasoning has a known and deterministic transition function, <i>P</i> </b>. This property dramatically simplifies the general form of the UBE, leading to a more direct and tractable equation for uncertainty propagation.
          </p>
          <p>
            Formally, we consider a <em>finite</em> horizon, finite state and action space MDP, with horizon length H &in; &Nopf;, state space &Sscr;, action space &Ascr; and rewards at time period h denoted by r<sup>h</sup> &in; &Ropf;. A policy &pi; = (&pi;<sup>1</sup>, &hellip;, &pi;<sup>H</sup>) is a sequence of functions where each &pi;<sup>h</sup> : &Sscr; &times; &Ascr; &rarr; &Ropf;₊ is a mapping from state-action pair to the probability of taking that action at that state, i.e., &pi;<sub>sa</sub><sup>h</sup> is the probability of taking action a at state s at time-step h and &sum;<sub>a</sub> &pi;<sub>sa</sub><sup>h</sup> = 1 for all s &in; &Sscr;. At each time-step h the agent receives a state s<sup>h</sup> and a reward r<sup>h</sup> and selects an action a<sup>h</sup> from the policy &pi;<sup>h</sup>, and the agent moves to the next state s<sup>h+1</sup>, which is sampled with probability P<sup>h</sup><sub>s&prime;sa</sub>. The Q-value, at time step h of a particular state under policy &pi; is the expected total return from taking that action at that state and following &pi; thereafter, i.e., Q<sup>&pi;,h</sup>(s,a) = <b>E</b> [ &sum;<sub>t=h</sub><sup>H</sup> r<sup>t</sup> | s<sup>t</sup> = s, a<sup>t</sup> = a, &pi; ]
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

</body>
</html>
