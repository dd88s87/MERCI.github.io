<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards">
  <meta name="keywords" content="Large Language Models, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fudan.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bibtex-js@2.0.0/dist/bibtex.min.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xuan Zhang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Ruixiao Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Zhijian Zhou</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              Long Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Yulei Qin</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Ke Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Xing Sun</a><sup>3</sup>
            </span>
            <span class="author-block">
              Xiaoyu Tan</a><sup>3*</sup>
            </span>
            <span class="author-block">
              Chao Qu</a><sup>1*</sup>
            </span>
            <span class="author-block">
              Yuan Qi</a><sup>1*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>3</sup>Tencent Youtu Lab</span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning has enabled LLMs to tackle challenging domains such as mathematics and code generation, but sparse rewards and inefficient exploration remain major bottlenecks. Existing methods like GRPO and DAPO remove the need for value functions and provide token-level variability, yet they lack temporally consistent exploration over long reasoning trajectories. Classic exploration strategies—such as entropy regularization or curiosity-driven rewards—struggle to scale to modern LLMs, either due to inefficiency, heavy computation, or reliance on heuristics.
          </p>
          <p>
            Our key insight is that autoregressive LLM reasoning operates in a deterministic environment: each token uniquely determines the next state. This dramatically simplifies the Uncertainty Bellman Equation, reducing the intractable challenge of estimating Q-value uncertainty to the more manageable task of estimating local reward uncertainty. Building on this, we introduce MERCI—the first principled deep exploration algorithm for LLM reasoning. MERCI leverages the lightweight and theoretically grounded Flipping Coins pseudo-count method to estimate state novelty, translating it into intrinsic rewards that guide policy optimization.
          </p>
        </div>
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>A Novel Theoretical Framework for LLM Exploration.</b> We establish a new framework based on a key insight: the LLM's known and deterministic transition dynamics simplify the Uncertainty Bellman Equation. This renders principled, uncertainty-driven exploration tractable at scale by reducing the intractable problem of Q-value uncertainty to a manageable estimation of local reward uncertainty.
            </li>
            <li>
              <b>A Practical and Scalable Exploration Algorithm.</b> We propose <b>MERCI</b>, a novel algorithm that operationalizes our theoretical framework. MERCI employs a highly scalable counting method to translate state novelty into a potent intrinsic reward signal, designed for seamless integration with modern, value-free policy optimization methods like GRPO.
            </li>
            <li>
              <b>State-of-the-Art Performance on Complex Reasoning.</b> Our extensive empirical evidence on challenging reasoning benchmarks, including MATH and SQL generation, demonstrate that MERCI beats strong baselines. Its directed exploration mechanism mitigates premature convergence and leads to the discovery of more robust and accurate solutions.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png"
                class="method-image"/>
          <p>Method Overview: MERCI</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<section class="section" id="references"> <div class="container is-max-desktop"> <h2 class="title is-3">References</h2> <ol id="ref-list" class="references"></ol> </div> </section>
<bibtex src="./static/bib/references.bib"></bibtex>

<div class="bibtex_template">
  <div class="reference">
    <span class="if author">
      <span class="author"></span>,
    </span>
    <span class="if year">
      (<span class="year"></span>).
    </span>
    <span class="if title">
      <b><span class="title"></span></b>.
    </span>
    <span class="if journal">
      <i><span class="journal"></span></i>,
    </span>
    <span class="if booktitle"><i><span class="booktitle"></span></i>,</span>
    <span class="if volume">vol. <span class="volume"></span>,</span>
    <span class="if pages">pp. <span class="pages"></span>,</span>
    <span class="if publisher"><span class="publisher"></span>,</span>
    <span class="if doi">doi: <a class="doi" target="_blank"></a>,</span>
    <span class="if url"><a class="url" target="_blank">[link]</a></span>
  </div>
</div>

</body>
</html>
