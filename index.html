<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards">
  <meta name="keywords" content="Large Language Models, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fudan.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bibtex-js@2.0.0/dist/bibtex.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xuan Zhang</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              Ruixiao Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Zhijian Zhou</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              Long Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Yulei Qin</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Ke Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Xing Sun</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Xiaoyu Tan</a><sup>3</sup><sup>†</sup>,
            </span>
            <span class="author-block">
              Chao Qu</a><sup>1</sup><sup>†</sup>,
            </span>
            <span class="author-block">
              Yuan Qi</a><sup>1,2</sup><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>3</sup>Tencent Youtu Lab</span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce <b>MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards)</b>, a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-4">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL) has enabled LLMs to tackle challenging domains such as mathematics and code generation, but sparse rewards and inefficient exploration remain major bottlenecks. Existing methods like GRPO and DAPO remove the need for value functions and provide token-level variability, yet they lack temporally consistent exploration over long reasoning trajectories. To guide exploration in such frameworks, many prevalent techniques rely on entropy regularization to encourage local policy diversity. While effective, this approach is limited for complex, long-horizon tasks. We see an opportunity to design complementary strategies that provide more directed, temporally-consistent exploration signals particularly for those tasks, motivating our investigation into principled exploration strategies compatible with modern value-free RL.
          </p>
          <p>
            Traditional exploration strategies—such as entropy regularization, curiosity-driven bonuses, or ensemble-based uncertainty estimates—either fail to provide temporally consistent signals, incur prohibitive computational costs, or break down at LLM scale. This fundamental mismatch between classic uncertainty quantification and the scale of LLMs necessitates a novel approach.
          </p>
          <p>
            Our work addresses this gap with a critical insight: autoregressive LLM reasoning operates in a deterministic environment: each token uniquely determines the next state. This dramatically simplifies the Uncertainty Bellman Equation, reducing the intractable challenge of estimating Q-value uncertainty to the more manageable task of estimating local reward uncertainty. Building on this, we introduce MERCI—the first principled deep exploration algorithm for LLM reasoning. MERCI leverages the lightweight and theoretically grounded <i>Flipping Coins pseudo-count</i> method to estimate state novelty, translating it into intrinsic rewards that guide policy optimization.
          </p>
          <p>
            To our knowledge, this is the first work to derive and apply a deep exploration algorithm for LLM reasoning directly from a principled simplification of the UBE. By recognizing that <b>the LLM serves as its own perfectly known world model</b>, we bridge the gap between model-aware RL theory and the typically model-free application of RL to LLMs. Our method integrates this simplified UBE framework with the ``Flipping Coins'' pseudo-count module to generate an <b>intrinsic reward</b>. This reward, expressed as an exploration bonus, guides policy optimization algorithms like GRPO to explore novel reasoning trajectories based on a coherent, temporally-consistent signal of epistemic uncertainty.  Experiments on complex reasoning benchmarks demonstrate that this approach significantly improves performance, effectively mitigating the tendency of standard algorithms to converge on repetitive and suboptimal solutions.
          </p>
        </div>
        <h3 class="title is-4">Contributions</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>A Novel Theoretical Framework for LLM Exploration.</b> We establish a new framework based on a key insight: the LLM's known and deterministic transition dynamics simplify the Uncertainty Bellman Equation. This renders principled, uncertainty-driven exploration tractable at scale by reducing the intractable problem of Q-value uncertainty to a manageable estimation of local reward uncertainty.
            </li>
            <li>
              <b>A Practical and Scalable Exploration Algorithm.</b> We propose <b>MERCI</b>, a novel algorithm that operationalizes our theoretical framework. MERCI employs a highly scalable counting method to translate state novelty into a potent intrinsic reward signal, designed for seamless integration with modern, value-free policy optimization methods like GRPO.
            </li>
            <li>
              <b>State-of-the-Art Performance on Complex Reasoning.</b> Our extensive empirical evidence on challenging reasoning benchmarks, including MATH and SQL generation, demonstrate that MERCI beats strong baselines. Its directed exploration mechanism mitigates premature convergence and leads to the discovery of more robust and accurate solutions.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png"
                class="method-image"/>
          <p style="text-align:center"><strong>Figure 1:</strong> Method Overview: MERCI</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Coin Flipping Network</h2>
        <div class="content has-text-justified">
          <p>CFN is a computationally efficient method of count-based exploration, which estimates a state's visitation count by solving a simple regression problem. The core idea is that a state's visitation count can be estimated by leveraging the statistical properties of the Rademacher distribution (i.e., random coin flips). The method works by setting up a supervised learning problem where a neural network \( f_\phi \), i.e., the CFN, is trained to predict the average of random coin-flip vectors associated with each state it encounters. </p>
          <p>For every visit to a state \(s_i\), a new random vector \(y_i\) (i.e., the <i>coin flips</i>) is sampled from \(\{-1, 1\}^d\). The CFN \(f_\phi\) is learned by solving \(\underset{\phi}{\arg\min} \mathbb{E}_{(s_i, y_i) \sim \mathcal{D}_{cfn}} [\mathcal{L}(s_i, y_i)]\), where \(\mathcal{L}\) is the mean-square error loss function and \(\mathcal{D}_{cfn}\) is a dataset of state-label pairs. Considering the fair coin-flip distribution \(\mathcal{C}\) over outcomes \(\{-1, 1\}\), we can flip this coin \(n\) times and average the results into \(z_n\).</p>
          <p>Specifically, the second moment of the sample mean \(z_n\) is related to the inverse count: \(\mathcal{M}_2(z_n) = \mathbb{E}[z_n^2] = \sum_i Pr(z_n = i) * i^2 = \frac{1}{n}\). \(\mathbb{E}[z_n^2]\) is the variance of the sample mean of the coin-flip distribution. Furthermore, by flipping <i>d</i> coins each time, the variance of \(z_n^2\) can be reduced by a factor of \(\frac{1}{d}\), which implies a reliable way for estimating the inverse count. To this end, we generate a <i>d</i>-dimensional random vector \(c_i \sim \{-1, 1\}^d\) as a label \(y_i\) for state \(s_i\). The learning objective is described as:</p>
          <p>
            \[
            f_\phi^*(s) = \underset{\phi}{\arg\min}\ \mathbb{E}_{(s_i, y_i) \sim \mathcal{D}_{cfn}} [\mathcal{L}(s_i, y_i)] = \underset{\phi}{\arg\min} \sum_{i=1}^{|\mathcal{D}_{\mathrm{cfn}}|} \| \mathbf{c}_i - f_\phi(s_i) \|^2.
            \]
          </p>
          <p>In the dataset \( \mathcal{D}_{cfn} \), each occurrence of the same state will be paired with a different random vector. </p>
          <p>\( f_\phi^* \) cannot learn a perfect mapping from states to labels since there are more than one (i.e., <i>m</i>) instances of the same state \(s_i\). Thus, it instead minimizes \(\mathcal{L}\) by outputting the mean random vector for all instances of a given state: \(f_\phi^*(s) = \frac{1}{n} \sum_{i=1}^n\mathbf{c}_i\). The pseudo-count can be estimated by:</p>
          <p>
            \[
            \frac{1}{d} \| f_{\phi}(s) \|^2 
            = \frac{1}{d} \sum_{j=1}^d \mathbb{E}\left[ \left( \sum_{i=1}^n \frac{c_{ij}}{n} \right)^2 \right] 
            = \frac{1}{d} \sum_{j=1}^d \mathbb{E}\left[ z_n^2 \right] 
            = \frac{1}{n}.
            \]
          </p>
          <p>By training \( f_\phi \) on the objective, we can map states to approximate the count by: \(\frac{1}{d}\|f_\phi(s)\|^2\approx \frac{1}{\mathcal{N}(s)}\), where \(\mathcal{N}(s)\) denote the counts of state <i>s</i>.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Methodology</h2>
        <h4 class="title is-4">The Uncertainty Bellman Equation with Known Transitions</h4>
        <div class="content has-text-justified">
          <!-- <p>
            The Uncertainty Bellman Equation (UBE) provides a principled mechanism for propagating epistemic uncertainty---quantified as the variance of the posterior distribution over Q-values---through time.  For clarity, we will use the terms "uncertainty" and "variance" interchangeably throughout this section. Our core theoretical contribution stems from a key insight: <b>the Markov Decision Process (MDP) underlying LLM reasoning has a known and deterministic transition function, <i>P</i></b>. This property dramatically simplifies the general form of the UBE, leading to a more direct and tractable equation for uncertainty propagation.
          </p>
          <p>
            Formally, we consider a <em>finite</em> horizon, finite state and action space MDP, with horizon length H &in; &Nopf;, state space &Sscr;, action space &Ascr; and rewards at time period h denoted by r<sup>h</sup> &in; &Ropf;. A policy &pi; = (&pi;<sup>1</sup>, &hellip;, &pi;<sup>H</sup>) is a sequence of functions where each &pi;<sup>h</sup> : &Sscr; &times; &Ascr; &rarr; &Ropf;₊ is a mapping from state-action pair to the probability of taking that action at that state, i.e., &pi;<sub>sa</sub><sup>h</sup> is the probability of taking action a at state s at time-step h and &sum;<sub>a</sub> &pi;<sub>sa</sub><sup>h</sup> = 1 for all s &in; &Sscr;. At each time-step h the agent receives a state s<sup>h</sup> and a reward r<sup>h</sup> and selects an action a<sup>h</sup> from the policy &pi;<sup>h</sup>, and the agent moves to the next state s<sup>h+1</sup>, which is sampled with probability P<sup>h</sup><sub>s&prime;sa</sub>. The Q-value, at time step h of a particular state under policy &pi; is the expected total return from taking that action at that state and following &pi; thereafter, i.e., Q<sup>&pi;,h</sup>(s,a) = <b>E</b> [ &sum;<sub>t=h</sub><sup>H</sup> r<sup>t</sup> | s<sup>t</sup> = s, a<sup>t</sup> = a, &pi; ].
          </p> -->
        <p>The Uncertainty Bellman Equation (UBE) provides a principled mechanism for propagating epistemic uncertainty---quantified as the variance of the posterior distribution over Q-values---through time.  For clarity, we will use the terms "uncertainty" and "variance" interchangeably throughout this section. Our core theoretical contribution stems from a key insight: <b>the Markov Decision Process (MDP) underlying LLM reasoning has a known and deterministic transition function, <i>P</i></b>. This property dramatically simplifies the general form of the UBE, leading to a more direct and tractable equation for uncertainty propagation.</p>
        
        <p>The UBE propagates Q-value uncertainty, \(U^h(s,a)\), defined as the posterior variance \( \mathbb{V}_t[\hat{Q}^{\pi, h}(s, a)] \), according to the following recursive inequality:</p>
        
        <p style="text-align: center;">\[ U^h(s, a) \leq \mathbb{V}_t[\hat{r}^h(s)] + \mathbb{E}_{\pi}[U^{h+1}(s', a')] \]</p>
        
        <p>where \( \mathbb{V}_t[\hat{r}^h(s)] \) is the immediate reward uncertainty, and the expectation is taken over the subsequent action \( a' \) dictated by the policy \(\pi\). This result reframes the complex task of estimating Q-value variance into the more tractable problem of estimating local reward uncertainty. Subsequently, \(U^h(s,a)\) is utilized to construct an <b>exploration bonus</b> (e.g., \(Q^{\pi,h}(s,a) + \alpha \sqrt{U^h(s,a)}\)) inspired by Upper Confidence Bound (UCB) principles, providing a theoretically grounded mechanism for balancing exploration and exploitation. Recognizing the difficulty of state-visitation counting in high-dimensional language state spaces, the method requires adopting a scalable pseudo-count mechanism to effectively estimate the local reward uncertainty \(\mathbb{V}_t[\hat{r}^h(s)]\).</p>
        </div>

        <h4 class="title is-4">Estimate Variance of Reware via CFN</h4>
        <div class="content has-text-justified">
          <p>
            Standard policy optimization driven by sparse, outcome-based rewards (e.g., GRPO) can lead to premature convergence on suboptimal solutions. MERCI addresses it via a dedicated mechanism for principled exploration. The framework is illustrated in Figure 1.
          </p>
          <p>
            Our framework employs two distinct Large Language Models operating in parallel:
            <ul>
              <li>
                <b>The Policy Network ( &pi;<sub>θ</sub>):</b> This is the agent that generates reasoning trajectories. It is initialized from a supervised fine-tuned (SFT) checkpoint, &pi;<sub>0</sub>, and its parameters θ are exclusively updated by the policy optimization algorithm (e.g., GRPO).
              </li>
              <li>
                <b>The CFN Network:</b> This network's sole purpose is to estimate epistemic uncertainty. It is a separate instance of the LLM, also initialized from the same checkpoint &pi;<sub>0</sub>. A lightweight MLP, which we call the CFN head <i>f</i><sub>&phi;</sub>, is attached to its final hidden layer. CFN network is updated together via a supervised regression objective.
              </li>
            </ul>
          </p>
          <p>
            The training process integrates these two networks as follows. During a training step, a reasoning trajectory is first generated by the current policy network &pi;<sub>θ</sub>. Each state (i.e., token) within this trajectory is then processed by the separate CFN network to extract its hidden representation s<sub>hidden</sub> and compute the variance of the reward through CFN head <i>f</i><sub>&phi;</sub> by \( \mathbb{V}[\hat{r}(s)]  = \frac{1}{d}\|f_\phi(s)\|^2 \).
          </p>
        </div>

        <h4 class="title is-4">Advantage Estimation</h4>
        <div class="content has-text-justified">
          <p><b>Calculating the Intrinsic Reward from Cumulative Uncertainty</b></p>
          <p>
            A critical detail of our method is the precise calculation of the exploration bonus. The correct approach to determine the uncertainty of a trajectory's value is to first at each step (we use the monte carlo estimation here), and only then take the square root of the total sum. This resulting value represents the standard deviation of the cumulative Q-value posterior and serves as our intrinsic reward.
          </p>
          <p>
            This stands in stark contrast to a common but theoretically flawed heuristic in many RL exploration algorithms. Those methods often compute a per-step bonus proportional to the local <i>standard deviation</i> and apply a standard RL algorithm to the modified, "bonused" rewards. However, this latter approach---which is equivalent to summing standard deviations---leads to a significant overestimation of uncertainty over long horizons. This miscalculation can cause the agent to become overly optimistic, leading to prolonged and inefficient exploration of paths that are long but not necessarily promising. To illustrate the difference, consider a trajectory of horizon <i>H</i> where the local reward variance at each step is \( \sigma^2=1 \). </p>
          </p>  
          <p> 
            <b>Correct Bonus (MERCI):</b> The cumulative variance is \( \sum_{h=1}^H 1 = H \). The bonus, or standard deviation, is correctly calculated as \( \sqrt{H} \). 
          </p>
          <p>
            <b>Heuristic Bonus:</b> The per-step bonus is \( \sqrt{1}=1 \). Summing these bonuses results in an overestimated total bonus of \( \sum_{h=1}^H 1 = H \).
          </p>
          <p>
            <b>MERCI</b> adheres strictly to the former, theoretically-grounded calculation, ensuring the exploration signal accurately reflects the true cumulative epistemic uncertainty.
          </p>
          <p></p>
          <p><b>Budget-Aware Exploration Bonus Control</b></p>
          <p>The non-sparse exploration bonus introduces its own considerable instabilities when becoming indiscriminately dense, which would invite LLMs seeking through aimless exploration. So, we enforce budgeted exploration, which reduces gradient variance and in turn stabilizes optimization and lowers noise in final answers. Concretely, three filtering stages are applied, shaping where and how the bonus can act. (1) <b>Percentile filtering</b> retains only a fixed fraction of the strongest signals within each sample, which tracks the gradual decline in bonus magnitude over training without manual retuning. (2) <b>Spatial coherence filtering</b> keeps tokens that belong to contiguous regions of elevated bonus and discards isolated spikes even when numerically large, thereby yielding steadier updates. (3) <b>Noise-suppression filtering</b> removes incentives attached to content that is unrelated to solving the problem, such as meaningless repetition, gratuitous code blocks, or rare characters generated solely to chase the bonus. Together these stages allocate a controlled exploration budget that preserves useful exploration while safeguarding the primary reward signal. The overall pipeline of bonus control is illustrated in Figure 2.</p>
          <div class="content has-text-justified">
            <img src="./static/images/filter.png"
                  class="filter-image"/>
            <p style="text-align:center"><strong>Figure 2:</strong> The entire pipeline of bonus filtering. Step 1: We rank all tokens within a response by their associated bonus values and retain only those falling within a predefined top percentile (e.g., the top 50% in this figure). Step 2: We only preserve clusters of adjacent tokens that consistently exhibit elevated bonuses (e.g., 3 consecutive tokens in this figure). Step 3: For example, in a math reasoning task without external tools, any Python code potentially generated during LLM rollouts is semantically irrelevant and noisy, so we exclude them from the overall bonus calculation.</p>
          </div>
          <p></p>
          <p><b>Advantage Normalization and Bonus Integration</b></p>
          <p>After bonus filtering, the normalized bonus \( \mathcal{B} \) is computed by first averaging the squared CFN outputs across all retained tokens and then applying square-root compression:</p>
          <p>
            \[
            \mathcal{B}=\sqrt{ \frac{1}{l}\sum_{i \in \mathbb{I}}\Big( \frac{1}{d}\|f_\phi(s_{hidden}^i)\|^2 \Big)},
            \]
          </p>
          <p>where \( l \) is the length of a trajectory, \( d \) is the dimension of CFN's outputs, and \( \mathbb{I} \) is the set of retained tokens' indices.</p>
          <p>
            To ensure comparability across trajectories sampled under the same prompt, we standardize trajectory-level bonuses within each group of size \(G\) and truncate negative values, preserving only positive exploratory incentives:
          </p>
          <p>
            \[
            \hat{A}_{\text{exploration}}^{i} 
            = \frac{\mathcal{B}_{i} - \mu}{\sigma},
            \text{where}
            \mu=\frac{1}{G}\sum_{j=1}^G \mathcal{B}_j,
            \ 
            \sigma=\sqrt{\frac{1}{G}\sum_{j=1}^G (\mathcal{B}_j-\mu)^2}.
            \]
          </p>
          <p>
            To prevent the bonus from overpowering outcome-based rewards, we scale the standardized intrinsic bonus term by an exploration coefficient \(\gamma]\), and add it to the base advantage \(\hat{A}_\text{old}^i\). For trajectories whose base advantage is negative, we cap the augmented advantage with a clipping factor \(\alpha \in (0,1)\) to prevent the intrinsic term from overwhelming the outcome signal:
          </p>
          <p>
            \[
            \hat{A}_{\text{new}}^i = \text{clip} \left( \hat{A}_{\text{old}}^i + \gamma \hat{A}_{\text{exploration}}^i, \alpha \hat{A}_{\text{old}}^i, (1+\alpha) \hat{A}_{\text{old}}^i \right)
            \]
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
      <div class="column is-seven-eighths">
        <h2 class="title is-4">Experiments</h2>
        <h3 class="title is-5">Experiment Setup</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>Mathematical Reasoning:</b> Our backbone model is Qwen2.5-Math-7B. Our training dataset is sourced from DAPO-17K, and we evaluate models on a diverse set of challenging mathematical reasoning benchmarks, including AIME2024/2025, MATH500, OlympiadBench, College Math, and Minerva.
            </li>
            <li>
              <b>SQL generation:</b> Our experiments are conducted on Llama-3.1-8B-Instruct. We trained on the Bird training set and evaluated performance on the Bird and Spider test sets.
            </li>
          </ul>
        <p></p>
        <div class="column is-seven-eighths">
          <h3 class="title is-5">Main Results</h3>
          <h4 class="title is-5">Coin Flipping Network</h4>
        </div>
        <p></p>
        <p></p>
        <img src="./static/images/cfn.png"
                class="cfn-image"/>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <p style="text-align:center"><strong>Figure 2:</strong> Some examples of token-level estimated epistemic uncertainty within a response. Red regions indicate relatively higher uncertainty estimates assigned by the CFN to the corresponding token positions, while blue regions indicate relatively lower estimates. We can observe that token sequences assigned <i>higher uncertainty</i> by the CFN predominantly correspond to <i>novel reasoning paths</i>, Python code along with its outputs, or specialized mathematical terminologies. This observation aligns with our hypothesis that more novel token positions tend to induce higher epistemic uncertainty and are therefore assigned higher values by our CFN. </p>
      </div>
      
      <h4 class="title is-5">Performance on Benchmarks</h4>
      <p></p>
      <div class="columns is-centered has-text-centered">
        <img src="./static/images/table1.png"
                class="table1-image"/>
      </div>
        <p></p>
      <div class="columns is-centered has-text-centered">
        <img src="./static/images/table2.png"
                class="table2-image"/>
      </div>
    <!--/ Abstract. -->
  </div>
</section>

</body>
</html>
