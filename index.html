<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards">
  <meta name="keywords" content="Large Language Models, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fudan.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bibtex-js@2.0.0/dist/bibtex.min.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xuan Zhang</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              Ruixiao Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Zhijian Zhou</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              Long Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Yulei Qin</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Ke Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Xing Sun</a><sup>3</sup>
            </span>
            <span class="author-block">
              Xiaoyu Tan</a><sup>3</sup><sup>†</sup>
            </span>
            <span class="author-block">
              Chao Qu</a><sup>1</sup><sup>†</sup>
            </span>
            <span class="author-block">
              Yuan Qi</a><sup>1,2</sup><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>3</sup>Tencent Youtu Lab</span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce <b>MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards)</b>, a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-4">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL) has enabled LLMs to tackle challenging domains such as mathematics and code generation, but sparse rewards and inefficient exploration remain major bottlenecks. Existing methods like GRPO and DAPO remove the need for value functions and provide token-level variability, yet they lack temporally consistent exploration over long reasoning trajectories. To guide exploration in such frameworks, many prevalent techniques rely on entropy regularization to encourage local policy diversity. While effective, this approach is limited for complex, long-horizon tasks. We see an opportunity to design complementary strategies that provide more directed, temporally-consistent exploration signals particularly for those tasks, motivating our investigation into principled exploration strategies compatible with modern value-free RL.
          </p>
          <p>
            Traditional exploration strategies—such as entropy regularization, curiosity-driven bonuses, or ensemble-based uncertainty estimates—either fail to provide temporally consistent signals, incur prohibitive computational costs, or break down at LLM scale. This fundamental mismatch between classic uncertainty quantification and the scale of LLMs necessitates a novel approach.
          </p>
          <p>
            Our work addresses this gap with a critical insight: autoregressive LLM reasoning operates in a deterministic environment: each token uniquely determines the next state. This dramatically simplifies the Uncertainty Bellman Equation, reducing the intractable challenge of estimating Q-value uncertainty to the more manageable task of estimating local reward uncertainty. Building on this, we introduce MERCI—the first principled deep exploration algorithm for LLM reasoning. MERCI leverages the lightweight and theoretically grounded <i>Flipping Coins pseudo-count</i> method to estimate state novelty, translating it into intrinsic rewards that guide policy optimization.
          </p>
          <p>
            To our knowledge, this is the first work to derive and apply a deep exploration algorithm for LLM reasoning directly from a principled simplification of the UBE. By recognizing that <b>the LLM serves as its own perfectly known world model</b>, we bridge the gap between model-aware RL theory and the typically model-free application of RL to LLMs. Our method integrates this simplified UBE framework with the ``Flipping Coins'' pseudo-count module to generate an <b>intrinsic reward</b>. This reward, expressed as an exploration bonus, guides policy optimization algorithms like GRPO to explore novel reasoning trajectories based on a coherent, temporally-consistent signal of epistemic uncertainty.  Experiments on complex reasoning benchmarks demonstrate that this approach significantly improves performance, effectively mitigating the tendency of standard algorithms to converge on repetitive and suboptimal solutions.
          </p>
        </div>
        <h3 class="title is-4">Contributions</h3>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>A Novel Theoretical Framework for LLM Exploration.</b> We establish a new framework based on a key insight: the LLM's known and deterministic transition dynamics simplify the Uncertainty Bellman Equation. This renders principled, uncertainty-driven exploration tractable at scale by reducing the intractable problem of Q-value uncertainty to a manageable estimation of local reward uncertainty.
            </li>
            <li>
              <b>A Practical and Scalable Exploration Algorithm.</b> We propose <b>MERCI</b>, a novel algorithm that operationalizes our theoretical framework. MERCI employs a highly scalable counting method to translate state novelty into a potent intrinsic reward signal, designed for seamless integration with modern, value-free policy optimization methods like GRPO.
            </li>
            <li>
              <b>State-of-the-Art Performance on Complex Reasoning.</b> Our extensive empirical evidence on challenging reasoning benchmarks, including MATH and SQL generation, demonstrate that MERCI beats strong baselines. Its directed exploration mechanism mitigates premature convergence and leads to the discovery of more robust and accurate solutions.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png"
                class="method-image"/>
          <p style="text-align:center"><strong>Figure 1:</strong> Method Overview: MERCI</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-3">Methodology</h2>
        <h4 class="title is-4">The Uncertainty Bellman Equation with Known Transitions</h4>
        <div class="content has-text-justified">
          <p>
            The Uncertainty Bellman Equation (UBE) provides a principled mechanism for propagating epistemic uncertainty---quantified as the variance of the posterior distribution over Q-values---through time.  For clarity, we will use the terms "uncertainty" and "variance" interchangeably throughout this section. Our core theoretical contribution stems from a key insight: <b>the Markov Decision Process (MDP) underlying LLM reasoning has a known and deterministic transition function, <i>P</i></b>. This property dramatically simplifies the general form of the UBE, leading to a more direct and tractable equation for uncertainty propagation.
          </p>
          <p>
            Formally, we consider a <em>finite</em> horizon, finite state and action space MDP, with horizon length H &in; &Nopf;, state space &Sscr;, action space &Ascr; and rewards at time period h denoted by r<sup>h</sup> &in; &Ropf;. A policy &pi; = (&pi;<sup>1</sup>, &hellip;, &pi;<sup>H</sup>) is a sequence of functions where each &pi;<sup>h</sup> : &Sscr; &times; &Ascr; &rarr; &Ropf;₊ is a mapping from state-action pair to the probability of taking that action at that state, i.e., &pi;<sub>sa</sub><sup>h</sup> is the probability of taking action a at state s at time-step h and &sum;<sub>a</sub> &pi;<sub>sa</sub><sup>h</sup> = 1 for all s &in; &Sscr;. At each time-step h the agent receives a state s<sup>h</sup> and a reward r<sup>h</sup> and selects an action a<sup>h</sup> from the policy &pi;<sup>h</sup>, and the agent moves to the next state s<sup>h+1</sup>, which is sampled with probability P<sup>h</sup><sub>s&prime;sa</sub>. The Q-value, at time step h of a particular state under policy &pi; is the expected total return from taking that action at that state and following &pi; thereafter, i.e., Q<sup>&pi;,h</sup>(s,a) = <b>E</b> [ &sum;<sub>t=h</sub><sup>H</sup> r<sup>t</sup> | s<sup>t</sup> = s, a<sup>t</sup> = a, &pi; ].
          </p>
        </div>

        <h4 class="title is-4">Estimate Variance of Reware via CFN</h4>
        <div class="content has-text-justified">
          <p>
            Standard policy optimization driven by sparse, outcome-based rewards (e.g., GRPO) can lead to premature convergence on suboptimal solutions. MERCI addresses it via a dedicated mechanism for principled exploration. The framework is illustrated in Figure 1.
          </p>
          <p>
            Our framework employs two distinct Large Language Models operating in parallel:
            <ul>
              <li>
                <b>The Policy Network ( &pi;<sub>θ</sub>):</b> This is the agent that generates reasoning trajectories. It is initialized from a supervised fine-tuned (SFT) checkpoint, &pi;<sub>0</sub>, and its parameters θ are exclusively updated by the policy optimization algorithm (e.g., GRPO).
              </li>
              <li>
                <b>The CFN Network:</b> This network's sole purpose is to estimate epistemic uncertainty. It is a separate instance of the LLM, also initialized from the same checkpoint &pi;<sub>0</sub>. A lightweight MLP, which we call the CFN head <i>f</i><sub>&phi;</sub>, is attached to its final hidden layer. CFN network is updated together via a supervised regression objective.
              </li>
            </ul>
          </p>
          <p>
            The training process integrates these two networks as follows. During a training step, a reasoning trajectory is first generated by the current policy network &pi;<sub>θ</sub>. Each state (i.e., token) within this trajectory is then processed by the separate CFN network to extract its hidden representation s<sub>hidden</sub> and compute the variance of the reward through CFN head <i>f</i><sub>&phi;</sub> by &Vopf;[r&#770;(s)] = (1/d) &#8741;f<sub>&phi;</sub>(s)&#8741;<sup>2</sup>.
          </p>
        </div>

      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-seven-eighths">
        <h2 class="title is-4">Experiments</h2>
        <h3 class="title is-5">Experiment Setup</h3>
        <div class="content has-text-justified"></div>
          <ul>
            <li>
              <b>Mathematical Reasoning:</b> Our backbone model is Qwen2.5-Math-7B. Our training dataset is sourced from DAPO-17K, and we evaluate models on a diverse set of challenging mathematical reasoning benchmarks, including AIME2024/2025, MATH500, OlympiadBench, College Math, and Minerva.
            </li>
            <li>
              <b>SQL generation:</b> Our experiments are conducted on Llama-3.1-8B-Instruct. We trained on the Bird training set and evaluated performance on the Bird and Spider test sets.
            </li>
          </ul>
        </div>
        <p></p>
        <p></p>
        <h3 class="title is-5">Main Results</h3>
        <h4 class="title is-5">Coin Flipping Network</h4>
        <img src="./static/images/cfn.png"
                class="cfn-image"/>
          <p style="text-align:center"><strong>Figure 2:</strong> Some examples of token-level estimated epistemic uncertainty within a response. Red regions indicate relatively higher uncertainty estimates assigned by the CFN to the corresponding token positions, while blue regions indicate relatively lower estimates. We can observe that token sequences assigned <i>higher uncertainty</i> by the CFN predominantly correspond to <i>novel reasoning paths</i>, Python code along with its outputs, or specialized mathematical terminologies. This observation aligns with our hypothesis that more novel token positions tend to induce higher epistemic uncertainty and are therefore assigned higher values by our CFN. </p>
        <h4 class="title is-5">Performance on Benchmarks</h4>
        <img src="./static/images/table1.png"
                class="table1-image"/>
        <p></p>
        <img src="./static/images/table2.png"
                class="table2-image"/>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

</body>
</html>
